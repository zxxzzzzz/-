
The links between the various web pages can be seen as a directed graph. The importance of a web page is to vote by linking to other pages on the page. A more linked page will have a higher level, whereas if a page has no chain entry or chain entry is lower, the higher the PR value of the page, the more important the page will be.

PageRank is a static algorithm that is independent of the query, so all PageRank values of the web pages can be obtained offline. This reduces the sorting time required to retrieve the user and greatly reduces the query response time. But PageRank has two drawbacks: first, the PageRank algorithm seriously discriminates against the newly added web pages, because the new web page links and links are usually very few, and the PageRank value is very low. PageRank algorithm only depend on the number of external links and other important degree to rank, while ignoring the correlation between the theme of the page, so that some topics are not related to web pages (such as AD pages) to obtain the bigger PageRank value, which affects the precision of the search results.

Because the original PageRank algorithm is not consider topics related factors, the department of computer science at Stanford Taher Haveli - wala presents a Sensitive Topic (Topic - Sensitive) of PageRank algorithm can solve the problem of "theme drift". This algorithm considers that some pages are considered important in some areas, but it is not important in other areas.
Page A links to page B, which can be regarded as the score of page A for page B. If web page A belongs to the same theme as page B, it can be considered that A is more reliable for B. Because A and B can be seen as peers, peers tend to know more about their peers than they do not, so their peers tend to be more reliable than their peers. Unfortunately TSPR does not use the relevance of the topic to improve the accuracy of link scoring.

As the search engine calculates web rankings, it relies heavily on links, and the quality of the links becomes increasingly important. In this case, the quality of the source site of the connection needs to be judged. And, more importantly, used to rely on links and correlation to determine the manner of ranking has met with various cheating provocation, Spam, led directly to Google had to find a new anti-cheating mechanism, to ensure the high quality site for search engines to attention. Sandbox and TrustRank have been mentioned in this situation. Intend to ensure that good sites get higher search performance and enhance site review. GuGe's own initial talk about TrustRank also mentions this.

Although TrustRank was originally used as a method of detecting waste, TrustRank's concept is more widely used in the current search engine ranking algorithm, which often affects the overall ranking of most websites. So TrustRank is really important and really worth paying attention to.

HillTop is the patent that a Google engineer, Bharat, acquired in 2001. HillTop is a query correlation link analysis algorithm which overcomes the shortcomings of PageRank's query independence. The HillTop algorithm says that links to related documents that have the same theme will have greater value to the searcher. In Hilltop, you're only thinking about those expert pages that are used to guide people through their resources. Hilltop in receives a query request, the first calculated according to the theme of the query a list of relevant experts page, and then according to the number of outside experts not affiliated with the affiliate page pointing to the target page, and ordered by relevance to the target page.
The HillTop algorithm has replaced the fact that the basic sorting of the web site and search keywords has replaced the value of relying too much on the value of PageRank to find those authority pages, and it avoids a lot of ways of trying to increase the value of PageRank by adding a lot of invalid links. The HillTop algorithm makes sure that the results of the evaluation and the relevance of the key word to the key word, the correlation to the key word, and the correlation between the different positions, and the number of phrases that can be used to prevent the number of words in the subject line.
However, experts of the page to search and determine the key effect to the algorithm, expert page quality plays a decisive role to the accuracy of the algorithm, also ignore the influence of most of the experts page. Experts page on the Internet accounts for the proportion of very low (1.79%), not on behalf of all the Internet web page, so the limitation to the HillTop. At the same time, unlike the PageRank algorithm, the algorithm of HillTop algorithm is run online and has great pressure on the response time of the system.


HillTop algorithm based on the "experts" document is the biggest difficulty is the first document "experts", from the current observation: Google apparently gave education first (edu), the government (. Gov) and non-profit organizations (. Org) site high priority. At run time: Google stores the index of the most searched keywords in large memory, so that the searcher continues to search for the same keyword phrase in the short term. The high frequency keywords there's another role in "Florida" many people have noticed before update: contains the spate of search keyword update frequency of the web site will be faster. As for: \ "SARS", there are millions of searches per day: Google will give priority to updating the site related to this topic.
Go back and look at each of the previous month "Google Dance", also can draw the following conclusion: Google is obviously for a keyword to give a random "weight", dynamic based on keyword query statistics found that these popular keywords, and then based on the HillTop algorithm for topic to find these pages containing keywords, make these pages as "experts" document of relevant keywords, according to these index update frequency of the entrance to keep high: this is obviously very effective for dealing with emergencies. Those pages with lower query frequencies may only be updated once a month. Simply put, Google will dynamically adjust the index strength of the corresponding site according to the hot level of the topic. However, the proportion of Google Chinese users in the total user and the total number of pages in the index of the Chinese web page in the index is also related to some extent.

In fact, the guiding ideology of HillTop algorithm and PageRank are consistent, which is to determine the ranking weight of search results by the number and quality of links. But HillTop think only calculated from relevant document links with the same theme will be bigger, the value of the searchers, namely the topic page links between for weighting the contribution of the higher than links aren't relevant at this value. If the site is to introduce "clothing", there are 10 links from websites related to "dress", that the 10 links than 10 other contributions from "appliances" related website links.. In 1999 and 2000, when the algorithm developed by Bharat and other Google developers, Bharat says the influential document of the subject matter of "experts" document, experts from these documents to the target page links to determine the "weighted score" the main part of the linked web pages.
Combining the HillTop algorithm with PageRank to determine the basic ordering process of the matching degree of web pages and search terms, instead of relying too much on PageRank's value to find the methods of those authoritative pages. The HillTop algorithm is very important in the sorting of two web pages with the same theme and similar PR. HillTop also avoids the temptation to increase web PageRank by adding many invalid links.

Google first HillTop algorithm to define the relevant web site: a web site with another site, in fact, the HillTop algorithm in Google as a recognition of cross-site link exchange interference (spam) and to identify similar technology. HillTop algorithm requirements: if there are two or more related to the theme of the website link to your website, then your web site in the search results will be a greater chance, if the HillTop algorithm does not find at least two correlation website, opportunity is the result of the search returns 0.
HillTop algorithm is actually rejected parts by using the method of random exchange links to disrupt the Google ranking rules and get good rankings, while in the HillTop paper also mentioned a lot about recognition of the union of web links exchange design: such as, according to the first three of the IPv4 address according to domain alias speculated that: 1
The PR value does not play a large role in the matching of search keywords: because of the high PR value in many websites containing non-related topics that contain the corresponding keywords. This is what GuGe is trying to avoid in the HillTop algorithm: it should do everything it can to list results related to search terms.
In general, from the past to today, many search engines have stopped the practice of using only one valuable algorithm to determine rankings. Such as: meta keyword tag, etc. This is just the beginning, and GuGe has completely ignored the meta tags in the HTML header in the first step. Compared with the non-visible meta tags, the visual part of a website USES the distraction technique less than the meta tag, because the visible part has to face the majority of actual visitors after all.


Google have server architecture is the grade distribution of ten thousand pentium server on the network. And once after understanding the Hilltop algorithm, it is hard to believe the pentium server can have such processing capacity: imagine, must first found in the thousands of thematic file file "experts", then the target web pages from these experts file link points, and then will return to Google numerical algorithm of other ranking system, and further processing - all of which is about 0.07 seconds - this let Google world-famous search speed to finish. It's incredible.

The search and determination of expert pages play a key role in the algorithm, and the quality of expert pages determines the accuracy of the algorithm. However, the quality and fairness of expert pages cannot be guaranteed to some extent. Hiltop ignored the impact of most non-expert pages.
In Hilltop's prototype system, the expert page occupies only 1 of the page. 79% do not fully reflect public opinion.
The Hilltop algorithm (less than two expert pages) is unable to get sufficient subset of expert pages (less than two expert pages), which means that Hilltop is best suited for sorting query sorting without overwriting. This means that Hilltop can be combined with some sort of page sorting algorithm to improve accuracy, rather than being an independent page-sorting algorithm.
According to the query subject from the collection of expert pages in Hilltop, the selected subset of the topic is also run online, which will affect the query response time as mentioned in the previous HITS algorithm. With the increase of the collection of expert pages, the scalability of the algorithm is deficient.

Google as early as February 2003, won the patent, but in the actual before put into use, the need to first ensure that the new algorithm and Google page rank and used by the correlation system full compatibility, so I need to do a lot to its compatibility tests, and then evaluate the result of the algorithm is provided after integration, seiko adjustment again, then is further complicated test... I think all of this will take a lot of time.

The search and determination of expert pages play a key role in the algorithm, and the quality of expert pages determines the accuracy of the algorithm. However, the quality and fairness of expert pages cannot be guaranteed to some extent. Hiltop ignored the impact of most non-expert pages.
In Hilltop's prototype system, the expert page occupies only 1 of the page. 79% do not fully reflect public opinion.
The Hilltop algorithm (less than two expert pages) is unable to get sufficient subset of expert pages (less than two expert pages), which means that Hilltop is best suited for sorting query sorting without overwriting. This means that Hilltop can be combined with some sort of page sorting algorithm to improve accuracy, rather than being an independent page-sorting algorithm.
According to the query subject from the collection of expert pages in Hilltop, the selected subset of the topic is also run online, which will affect the query response time as mentioned in the previous HITS algorithm. With the increase of the collection of expert pages, the scalability of the algorithm is deficient.

The HITS (Hyperlink Induced Topic Search) algorithm, which was proposed by Kleinberg in 1998, is one of the most famous algorithms in the Hyperlink analysis sorting algorithm. The algorithm, in the direction of hyperlinks, divides the page into two types of pages: the Authority page and the Hub page. Authority page page, also called Authority, it refers to a query keywords and combination of the most relevant pages, Hub page is also called the page directory, the page content is mainly a large number of links to the page Authority, its main function is to make the Authority pages together. For the Authority page P, the more the Hub pages that point to P, the higher the quality, the greater the value of P's Authority; For Hub page H, the more pages of Authority that H points to, the higher the quality of the Authority page, and the greater the Hub value of H. For the entire Web set, the Authority and Hub are interdependent, mutually reinforcing, and mutually reinforcing relationships. The optimal relationship between Authority and Hub is the basis of HITS algorithm.
HITS's basic idea is that the algorithm measures the importance of a web page based on the degree of entry of a web page (the hyperlink to this page) and the degree of (from this page to another page). After the limits according to the web page out of and into the degrees to establish a matrix, by iterative computation of the matrix and defines the threshold of convergence to two vectors Authority and Hub values updated until convergence.
Experimental data show that HITS ranking high accuracy than PageRank, HITS algorithm design web users to evaluate the quality of the network resources common standard, therefore to be able to better use of network information retrieval tools for the user to access the Internet resources.
However, it has the following defects: first, the HITS algorithm only computes the main feature vector, which can not handle the problem of topic drift. Secondly, the topic generalization can be generated when the topic query is narrow. Thirdly, the HITS algorithm can be said to be an experimental experiment. It must be calculated based on the content retrieval results page and the links between the pages directly linked after the network information retrieval system is conducted for the content retrieval operation. Although some attempts to the Server through the calculation of algorithm improvement and set up link structure (Connectivity Server), and other operations, can achieve a certain degree of online real-time computation, but its computation cost is still not acceptable.
Sorting algorithms are particularly important in search engines, and many search engines are now exploring new sorting methods to improve user satisfaction. However, there are two deficiencies in the second generation search engine. In this context, the third generation search engine based on intelligent sorting is also born.
1) relevance
Relevance refers to the degree of correlation between the search term and the page. Due to the complexity of language, it is one-sided to judge the correlation between the search terms and the page only through link analysis and the surface characteristics of the web page. For example, "rice blast" was retrieved, and there was a web page to introduce the information of rice diseases and pests, but there was no "rice blast" in the paper, and the search engine could not retrieve it at all. For these reasons, there is no solution to the large number of search engine cheating. The method to solve the correlation should be to increase semantic understanding, analyze the relevant degree of keywords and webpage, and the more accurate the correlation analysis, the better the user's search effect will be. At the same time, low relevance pages can be removed to effectively prevent the search engine from cheating. The correlation between keywords and web pages is run online, which can give the system a great deal of time. The distributed architecture can improve the scale and performance of the system.
2) simplification of search results
On the search engine, anyone searches for the same word. This does not satisfy the user's needs. Different users have different requirements for retrieval results. Ordinary farmers, for example, retrieve the "blast", just want to get information of rice blast and the method of prevention and cure, but agriculture experts or technology workers might want to have the papers related to rice blast.
One way to solve the search results is to provide personalized service for intelligent search. Through Web data mining, user models (such as user background, interest, behavior, style) are established to provide personalized services..
